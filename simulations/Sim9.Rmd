---
title: "Sensemaking Simulation 9: Pruning irrelevant causal nodes."
output:
  html_document:
    df_print: paged
---




```{r}
source("../src/definitions.R")


if(exists("seed"))
{
    seed <- seed + 1
} else {
    seed <- 100
}
#seed <- 136
cat("Using random seed: ",seed,"\n")
set.seed(seed)

test <- 9

```
## About

This simulation integrates the link pruning and the delta-rule learning.  It uses the same
initial model as 7/8, which has a lot of non-influencing features.   Then, periodically during learning, 
it will examine the hypothesis space and prune irrelevant features.  In addition, it embeds a notion of recent history in terms of a short-term
memory.  The model evaluation needs to evaluate prospective models against data, and this restricts it to a relatively small look-back, instead of the entire data set.

The model-evaluation essentially computes standardized beta weights, which require estimates of the variability of the data
(predictors and outcome).  Because these are standardized, we use an absolute criterion; if the ensemble is all within a small
range of 0, then the node is not doing anything and we can get rid of it.


## Setup

To set up an environment, you need:

1. To specify nodes
2. To specify connections between nodes (aweights)
3. To specify values of nodes (avals)
4. to specify input, output, and 'visible' nodes.
5. Specify initial hypothesis space (hvals/hweights/hframe,houtput)


```{r}

  ##input nodes and 1 output node.
  ## there are only 6 relevant nodes, the rest are garbage.
  ##test 8 is the random contrast condition for the same set-up
  ##test 9 is a crafted constrast condition.
  
  numnodes<- 40
  
  ##learn from.
  
  avals <- runif(numnodes)
  ##each row represents the outgoing connections from each node
  aweights <-array( matrix(rep(0,numnodes^2),numnodes),
    dim=list(numnodes,numnodes,1))
  aweights[,numnodes,1] <- c(-10,-5,-1,rep(0,numnodes-7),1,5,10,0)
  input <- c(rep(T,numnodes-1),F)
  output <- !input
  visible <- rep(T,numnodes)
  
coord <-    plotMM(aweights[,,1],aweights[,,1],input,output) ##take 
  
```



# Generate the data:

```{r}

data <- generateData(reps=1000,aweights=aweights,avals=avals,iter=10,input=input,noise=noise)

```


## The main simulation/learning phase.

```{r echo=FALSE,message=FALSE,warning=FALSE}
#set.seed(as.numeric(Sys.time()))
printme <- FALSE
numExemplars <- 10      ##how many exemplars
exemplarsToUpdate <- 5
simreps <- 500
trial <- 1

stmSize <- 5
stm <- matrix(NA,nrow=stmSize,ncol=numnodes)

##implement learning here.
alpha <- .25 #learning rate.
decay <- 1.0
maxback <- 10  #how deeply to simulate causal model/back-propogate.


reframecycle <- 50 ##how often to engage in 'reframing'



startingnoise <- 1


  ##start at complete random weights.
  hframe <- matrix(rep(0,numnodes^2),numnodes)
  hframe[1:(numnodes-1),numnodes] <- 1#
  #hframe <- generateRandomMentalModel(5,input,output)#
 # hframe <- aweights[,,1]!=0 ##pick out the correct mental model:
  hframe[24:27,numnodes] <- 0  ##get rid of irrelevant nodes immediately.
  #hframe[4:27,numnodes] <- 0  ##get rid of irrelevant nodes immediately.
  diag(hframe) <- 0

  houtput <- output#!rowSums(hframe!=0)
  


##now, set up the ensemble of hypothesezs about the frame.
hweights <- array(rep(hframe,numexemplars)* (startingnoise -2*startingnoise*runif(numnodes^2*numexemplars)),dim=list(numnodes,numnodes,numexemplars))
##estimate mean error of model throughout, for each node.
meanerror <- matrix(0,nrow=numexemplars,ncol=nrow(hframe))

ord <- sample(nrow(data))
##this repeats a random order  enough times to fill in simreps. Same order each time.
simtrials <- rep(ord,ceiling(simreps/length(ord)))[sample(1:simreps)]
learningerror <- array(NA,
                       dim=list(exemplarsToUpdate,numnodes,simreps))
trials <- ord
counter <- 1


  par(mfrow=c(1,3),mar=c(0,3,3,0))
  ##Do the true world
#  textcol <- c("black","white","white")[1+input + 2*output]
  plotMM(aweights[,,1],aweights[,,1],input,output,coords=coord) ##take coordinates from earlier
  title("Veridical Causal Model Structure",line=-1)
  plotMM(hframe,apply(hweights,c(1,2),mean),input,houtput,coords=coord)
  title("Mental model structure",line=-1)
   
  plotWeights(hweights,hframe,output,main=paste("Trial: 0"))


prevdata <- data[1,]
truth <- prevdata
observed <- prevdata
observed[houtput] <- NA
for(i in simtrials)##go through this many examples of data.
{
  exemplars <- sample(numexemplars)[1:exemplarsToUpdate]  ##pick, e.g., five at a time:
  truth <- data[i,]  #what is really the case
  stm[(trial-1)%%stmSize+1,] <- truth  ##populate a short-term memory history. 
  observed <- truth  ##what the observed truth is.
  observed[houtput]<- NA

  
##this is predicting based ONLY on the input nodes.

iter <- 1
##pick which node to use (of the output node(s))
node <- mysamp(houtput,length(houtput))


##now, we have made a set of predictions for 
##pick an output node and back-propogate error.

while(iter < maxback)
{
  if(printme) cat("Node:",node ,"| ")

    filter <-  !((1:length(output)) == node ) &!output


  ##make a prediction of each missing value, for each model.
  predictions <- simulateEnsemble(hweights[,,exemplars,drop=FALSE],
                                  observed,iter=10,
                                  inputnodes=filter, ##input
                                  noise=rep(0.0001,length(input)))  


  ##only go through all this if we have a real problem:
    inp <- hframe[,node]
  if(sum(abs(inp))>0)
  {
    ##for each model, get the error for this node's prediction:
    delta2 <- t(truth -(predictions))[,node] #here is an error signal, comparing to all known true values.
    ##^^^^^this is the amount we were off, for each model.
    
    ##Calculate the input weights of the exemplar models
    weights <- t(hweights[,node,exemplars])  #* hframe[,node] ##filter weigths by frame.
    #^^^^  these are the original weights we need to update.

    
    ##change is how much each weight needs to be adjusted.
  #  change <-  t(alpha * (delta2) * truth * t(abs(sign(weights)))) ##abs(sign(weights)) forces output to 0 and preserves exemplars.
    change <- t(alpha * truth %*% t(delta2) * t(abs(sign(weights))))
    weights <- weights + change


    hweights[,node,exemplars] <- t(weights)
  

   ##now, move backward no one of the incoming nodes
   node <- mysamp(inp>0,1)
#   iter <- maxback
   learningerror[,,counter] <- (delta2)
   counter <- counter + 1
  } else{
    if(printme) cat ("Maximum backprop\n")
    iter <- maxback
  }

    
   iter <- iter + 1  
}

##now, let's calculate whether to reframe things.
    if(trial %% reframecycle == 0)
    {
      
      ##first, can we get rid of any terrible ensemble members? Let's get rid of the worst and replace it with the best.
      
    
      
      
      
      
      ev <- evaluateMentalModel(stm,hweights,meanerror,
                      observed,
                      input,houtput,
                      iter=10,noise=rep(0.01,numnodes),plot=F)

  ##select the links (not nodes) to remove now; so we don't add/remove the same node later.
  remove <- ev$removeVar & !outer(houtput,houtput,"&")


  ##remove any nodes recommended by evaluatemodel from the frame.
  hframe[remove] <- 0
  
  ##zero out the weights:
  for(i in 1:numExemplars)
    hweights[,,i][remove] <- 0
  
  
  par(mfrow=c(1,3),mar=c(0,3,3,0))
  ##Do the true world
#  textcol <- c("black","white","white")[1+input + 2*output]
  plotMM(aweights[,,1],aweights[,,1],input,output,coords=coord) ##take coordinates from earlier
  title("Veridical Causal Model Structure",line=-1)
  plotMM(hframe,apply(hweights,c(1,2),mean),input,houtput,coords=coord)
  title("Mental model structure",line=-1)
  
  plotWeights(hweights,hframe,output,main=paste("Trial:",trial))

    }
    
  alpha <- alpha * decay
 trial <- trial + 1
 
 prevdata <- truth##update memory for prior trial.

}
```


```{R}
par(mfrow=c(1,3),mar=c(3,4,2,0))
  image(as.matrix(hweights[,numnodes,]),col=brewer.pal(11,"RdYlGn"))
  plotWeights(hweights,hframe,output,main=paste("Trial:",trial))

 lerr <- apply(abs(learningerror),3, mean)
  plot(lerr,col="grey30",main="Error over time",ylab="Mean absolute error",xlab="Learning round")
  points(lowess(lerr,f=.2),lty=1,type='l',lwd=4,col="red")

```

