---
title: "Sensemaking Simulation 7/8"
date: "`r Sys.Date()`"
knit: (function(inputFile, encoding) { 
      rmarkdown::render(inputFile,
                        encoding=encoding, 
                        output_dir = "../sim-output")})
output:
  html_document:
    df_print: paged
---

```{r}
source("../src/definitions.R")


if(exists("seed"))
{
    seed <- seed + 1
} else {
    seed <- 100
}
#seed <- 136
cat("Using random seed: ",seed,"\n")
set.seed(seed)

##  7/8: exploring contrast in learning

test <- 7

```


## About:

This is documented in 2019 report as model #2, which is really model 7 in this file.
Its purpose is to explore how adding variability to initial estimates will impact learning.


Model 8/9 were experiments that never really panned out that explored contrast.


## Setup

To set up an environment, you need:

1. To specify nodes
2. To specify connections between nodes (aweights)
3. To specify values of nodes (avals)
4. to specify input, output, and 'visible' nodes.
5. Specify initial hypothesis space (hvals/hweights/hframe,houtput)





## The main simulation/learning phase.
```{r fig.width=8,fig.height=6,echo=FALSE,message=FALSE,warning=FALSE}

if(test==7 | test==8)
{ 
  ##input nodes and 1 output node.
  ## there are only 6 relevant nodes, the rest are garbage.
  ##test 8 is the random contrast condition for the same set-up
  ##test 9 is a crafted contrast condition.
  
  numnodes<- 15
  
  ##learn from.
  
  avals <- runif(numnodes)
  ##each row represents the outgoing connections from each node
  aweights <-array( matrix(rep(0,numnodes^2),numnodes),
    dim=list(numnodes,numnodes,1))
  aweights[,numnodes,1] <- c(-10,-10,-10,rep(0,numnodes-7),10,10,10,0)
  input <- c(rep(T,numnodes-1),F)
  output <- !input
  visible <- rep(T,numnodes)
  
  
  
  
  hvals <- avals
  ##start at complete random weights.
  hframe <- matrix(rep(0,numnodes^2),numnodes)
  hframe[1:(numnodes-1),numnodes] <- rnorm(numnodes-1)
  #hframe <- generateRandomMentalModel(5,input,output)#
 # hframe <- aweights[,,1]!=0 ##pick out the correct mental model:
  diag(hframe) <- 0

  houtput <- output#!rowSums(hframe!=0)
  
  
}
```

## Setup complete. Display the true and mental-model environment




## The main simulation/learning phase.
```{r fig.width=8,fig.height=6,echo=FALSE,message=FALSE,warning=FALSE}

  par(mfrow=c(1,2),mar=c(0,0,0,0))
  ##Do the true world
#  textcol <- c("black","white","white")[1+input + 2*output]
  coord <- plotMM(aweights[,,1],aweights[,,1],input,output)
  title("Veridical Causal Model Structure",line=-1)
  plotMM(aweights[,,1],hframe,input,houtput,coord=coord)
  title("Mental model structure",line=-1)

```
# Generate the data:

```{r}

data <- generateData(reps=1000,aweights=aweights,avals=avals,iter=10,input=input,noise=noise)


##also, generate test data under other circumstances.
##we could, for example, test the model on a different
##true world model and see how it does.

noise <- rep(.05,numnodes)  ##set this higher and the whole thing falls apart.
##Here, create a second set of aweights and data.

avals2 <- avals
avals2[!input] <- NA

##Now, a data set of just the input nodes has been created.
##each row represents an independent case.

aweights2 <- aweights
if(test==6)
{
  aweights2[1,5,1] <-   .2
  aweights2[2,5,1] <-  -.2
  aweights2[3,5,1] <-   -2
  aweights2[4,5,1] <-   2
  
}
data2 <- generateData(reps=1000,aweights=aweights2,avals=avals2,iter=10,input=input,noise=noise)
  
```


## The main simulation/learning phase.

```{r echo=FALSE,message=FALSE,warning=FALSE, fig.width=9,fig.height=4}
#set.seed(as.numeric(Sys.time()))
printme <- FALSE
numexemplars <- 10      ##how many exemplars
exemplarsToUpdate <- 5
simreps <- 250
trial <- 1


##implement learning here.
alpha <- .25 #learning rate.
decay <- 1.0
maxback <- 10



learningerrors <- list()


index <- 1
for(startingnoise in c(.1,1,10,50,100))
{

##now, set up the ensemble of hypothesezs about the frame.
  
##we will have the ensemble initial estimates average the truth, with more or less noise around that.
  
  
hweights <-  + array(rep(aweights,numexemplars) + rep(aweights !=0,numexemplars)* (startingnoise -2*startingnoise*runif(numnodes^2*numexemplars)),dim=list(numnodes,numnodes,numexemplars))

##estimate mean error of model throughout, for each node.
meanerror <- matrix(0,nrow=numexemplars,ncol=nrow(hframe))

ord <- sample(nrow(data))
##this repeats a random order  enough times to fill in simreps. Same order each time.
simtrials <- rep(ord,ceiling(simreps/length(ord)))[sample(1:simreps)]
learningerror <- array(NA,
                       dim=list(exemplarsToUpdate,numnodes,simreps))
trials <- ord
counter <- 1

prevdata <- data[1,]
truth <- prevdata
observed <- prevdata
observed[houtput] <- NA
for(i in simtrials)##go through this many examples of data.
{
  if(test==8)
  {
    prevtruth <- truth
    prevobserved <- observed
  }
  exemplars <- sample(numexemplars)[1:exemplarsToUpdate]  ##pick, e.g., five at a time:
  truth <- data[i,]  #what is really the case
  observed <- truth  ##what the observed truth is.
  observed[houtput]<- NA

  
##this is predicting based ONLY on the input nodes.

iter <- 1
##pick which node to use (of the output nodes)
node <- mysamp(houtput,length(houtput))


##now, we have made a set of predictions for 
##pick an output node and back-propogate error.

while(iter < maxback)
{
  if(printme) cat("Node:",node ,"| ")

    filter <-  !((1:length(output)) == node ) &!output


  ##make a prediction of each missing value, for each model.
  predictions <- simulateEnsemble(hweights[,,exemplars,drop=FALSE],
                                  observed,iter=10,
                                  inputnodes=filter, ##input
                                  noise=rep(0.0001,length(input)))  

if(test==8){
  prevpredictions <- simulateEnsemble(hweights[,,exemplars,drop=FALSE],
                                  prevobserved,iter=10,
                                  inputnodes=filter, ##input
                                  noise=rep(0.0001,length(input)))  
}
  
  ##only go through all this if we have a real problem:
    inp <- hframe[,node]
  if(sum(abs(inp))>0)
  {
    ##for each model, get the error for this node's prediction:
    delta2 <- t(truth -(predictions))[,node] #here is an error signal, comparing to all known true values.
    ##^^^^^this is the amount we were off, for each model.
    
    ##Calculate the input weights of the exemplar models
    weights <- t(hweights[,node,exemplars]) 
    #^^^^  these are the original weights we need to update.

    
    ##change is how much each weight needs to be adjusted.
  #  change <-  t(alpha * (delta2) * truth * t(abs(sign(weights)))) ##abs(sign(weights)) forces output to 0 and preserves exemplars.
    change <- t(alpha * truth %*% t(delta2) * t(abs(sign(weights))))
    
       # change[,node] <- 0
    if(test==8)
    {
      ##test 8 uses contrast. Here, we update weights to move toward the double-delta we 
      #just saw:
      
      #delta2.old <-  t(prevtruth -(prevpredictions))[,node]
      #change.old <-   alpha * (delta2.old) * prevtruth * abs(sign(weights))
      
      #ddelta <- delta2.old-delta2
      #dchange <- change.old-change
      #dweight <- - dchange/ddelta
      dtruth <- (prevtruth-truth)
      ##normalize so the change in output node is 1.0:
      
      ##this weights these things:
      #sfdtruth <- dtruth2^(gamma)/sum(dtruth2^gamma,na.rm=T)
      #sfdtruth <- sfdtruth / mean(sfdtruth,na.rm=T) #make mean 1.0, so that 7=8 if gamma=0
      #sfdtruth[is.na(sfdtruth)] <- 0
      
      dtruth[node] <- 0
      worst <- order(abs(dtruth),decreasing=T)[6:length(dtruth)]
      change[,worst] <- 0 
      
      ##find the subset of nodes that have the largest difference in input values
      
      
 
      weights <- weights + change
      
    }else{
      weights <- weights + change
    }


  hweights[,node,exemplars] <- t(weights)
  

   ##now, move backward no one of the incoming nodes
   node <- mysamp(inp>0,1)
#   iter <- maxback
   learningerror[,,counter] <- (delta2)
   counter <- counter + 1
  } else{
    if(printme) cat ("Maximum backprop\n")
    iter <- maxback
  }

   iter <- iter + 1  
}
  alpha <- alpha * decay
 trial <- trial + 1
 
 prevdata <- truth##update memory for prior trial.

 
 
}

if(test==7|test==8)
{
 par(mfrow=c(1,2),mar=c(6,5,2,0))
#  image(as.matrix(hweights[,numnodes,]),col=brewer.pal(11,"RdYlGn"))
  matplot(hweights[,numnodes,],type="b",pch=16,lty=3,col="black",bty="L",
          xlab="Feature index",ylab="Weight",xaxt="n",main=paste("Final parameter estimates\nInitial noise:",startingnoise))
  axis(1,1:15)
 lerr <- apply(abs(learningerror),3, mean)
  plot(lerr,col="grey30",main="Error over time",ylab="Mean absolute error",xlab="Learning round",bty="L",
       xlim=c(0,250),ylim=c(0,50))
  
  points(lowess(lerr,f=.2),lty=1,type='l',lwd=4,col="red")
}


learningerrors[[index]] <- learningerror
index <- index + 1
}
```

```{r}
##this will temporarily keep track of a learning sequence/simulation.
lerr_.1 <- learningerrors[[1]]
lerr_1 <-  learningerrors[[2]]
lerr_10 <-  learningerrors[[3]]
lerr_50 <-  learningerrors[[4]]
lerr_100 <-  learningerrors[[5]]
```





```{r fig.width=7,fig.height=5,echo=FALSE,message=FALSE,warning=FALSE}
if(TRUE)
{
 par(mfrow=c(1,1),mar=c(6,5,2,0))
#  image(as.matrix(hweights[,numnodes,]),col=brewer.pal(11,"RdYlGn"))

  plot(0,0, type="n",col="grey30",main=paste("Error over time"),
       ylab="Mean absolute error",xlab="Learning epoch",bty="L",
       xlim=c(-10,500),ylim=c(0,60))

 lerr <- apply(abs(lerr_.1),3, mean)
 points(lowess(lerr,f=.2),lty=1,type='l',lwd=4,col="red")

 lerr <- apply(abs(lerr_1),3, mean)
 points(lowess(lerr,f=.2),lty=1,type='l',lwd=4,col="red")
lerr <- apply(abs(lerr_10),3, mean)
 points(lowess(lerr,f=.2),lty=1,type='l',lwd=4,col="red")

lerr <- apply(abs(lerr_50),3, mean)
 points(lowess(lerr,f=.2),lty=1,type='l',lwd=4,col="red")

  lerr <- apply(abs(lerr_100),3, mean)
 points(lowess(lerr,f=.2),lty=1,type='l',lwd=4,col="red")

 text(0,c(5,7,9,28,55), c(.1,1,10,50,100),pos=2,cex=.6)
 }



if(TRUE)
{
 par(mfrow=c(1,1),mar=c(6,5,2,0))
#  image(as.matrix(hweights[,numnodes,]),col=brewer.pal(11,"RdYlGn"))

  plot(0,0, type="n",col="grey30",main=paste("Error over time"),
       ylab="Mean absolute error",xlab="Learning epoch",bty="L",
       xlim=c(-10,550),ylim=c(0,120))

  
 lerr <- apply(abs(lerr_.1),3, mean)
 points(lerr,pch=16,cex=.5,col="red")
 points(lowess(lerr,f=.2),lty=1,type='l',lwd=4,col="red")

 lerr <- apply(abs(lerr_1),3, mean)
 points(lerr,pch=16,cex=.5,col="orange")
  points(lowess(lerr,f=.2),lty=1,type='l',lwd=4,col="orange")

 lerr <- apply(abs(lerr_10),3, mean)
 points(lerr,pch=16,cex=.5,col="yellow")
 points(lowess(lerr,f=.2),lty=1,type='l',lwd=4,col="yellow")

lerr <- apply(abs(lerr_50),3, mean)
 points(lerr,pch=16,cex=.5,col="green4")
 points(lowess(lerr,f=.2),lty=1,type='l',lwd=4,col="green4")

lerr <- apply(abs(lerr_100),3, mean)
 points(lerr,pch=16,cex=.5,col="blue")
 points(lowess(lerr,f=.2),lty=1,type='l',lwd=4,col="blue")

#  lerr <- apply(abs(lerr.5),3, mean)
# points(lowess(lerr,f=.2),lty=1,type='l',lwd=4,col="blue")
#rect(500,0,550,10,border="white",col="white")
#text(c(500,500,500,500),c(3,1,0,4), c(.05,.1,".2/.3",.4),pos=4,cex=.6)

 
 }


```


```{r fig.width=7,fig.height=5,echo=FALSE,message=FALSE,warning=FALSE}
if(1)
{
 par(mfrow=c(1,1),mar=c(6,5,2,0))
#  image(as.matrix(hweights[,numnodes,]),col=brewer.pal(11,"RdYlGn"))
  lerr <- apply(abs(learningerror),3, mean)
  plot(lerr,col="grey30",main=paste("Error over time\n","noise=+/-",startingnoise),
       ylab="Mean absolute error",xlab="Learning round",bty="L",
       xlim=c(0,1600))
  
  points(lowess(lerr,f=.2),lty=1,type='l',lwd=4,col="red")
}
```



```{r}
if(1)
{
  par(mfrow=c(1,1))

  ##this examines the 'output' values, predicted by the 'input' values, 
  ##for a given fuzzy mental model.
  ev <- evaluateMentalModel(data,hweights,meanerror,
                      observed,
                      input,houtput,
                      iter=10,noise=rep(0.01,numnodes),plot=T)
  print(ev$remove)

  ev2 <- evaluateMentalModel(data2,hweights,meanerror,
                              observed,
                              input,houtput,
                              iter=10,noise=rep(0.01,numnodes),plot=T)
  
cat("Percent of observed data that are consistent with model:", (mean(ev$keep)),"\n")
cat("percent consistent:",mean(ev$consistent),"\n")
cat("Percent of observed data that are consistent with converged model:", (mean(!ev$consistent | (ev$consistent&!ev$keep))),"\n")


cat("Percent of observed data that are consistent with model:", (mean(ev2$keep)),"\n")
cat("percent consistent:",mean(ev2$consistent),"\n")

cat("Percent of observed data that are consistent with converged model:", (mean(!ev2$consistent | (ev2$consistent&!ev2$keep))),"\n")

cols <- colorRampPalette(c("gold","black"))(10)
par(mfrow=c(1,2),mar=c(5,3,1,1))
matplot(data[,5],ev$pdata[,5,],pch=16,xlab="Observed data",ylab="Model predictions",main="True data",col=cols)
abline(0,1,lwd=3)
abline(2*mean(meanerror[,5]),1)
abline(-2*mean(meanerror[,5]),1)

matplot(data2[,5],ev2$pdata[,5,],pch=16,xlab="Observed data",ylab="Model predictions",main="Improper data",col=cols)
abline(0,1,lwd=3)
abline(2*mean(meanerror[,5]),1)
abline(-2*mean(meanerror[,5]),1)

}
```
